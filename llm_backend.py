# llm_backend.py
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Bio-ML Agent â€” Ã‡oklu LLM Backend DesteÄŸi
#  Ollama, OpenAI, Anthropic ve Gemini backend'lerini destekler.
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

from __future__ import annotations

import json
import logging
import os
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional

log = logging.getLogger("bio_ml_agent")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Abstract Base Class
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class LLMBackend(ABC):
    """TÃ¼m LLM backend'lerin temel sÄ±nÄ±fÄ±.

    Her backend `chat()` metodunu uygulamalÄ±dÄ±r.
    """

    name: str = "base"

    @abstractmethod
    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """Mesaj listesi gÃ¶nderip yanÄ±t al.

        Args:
            messages: OpenAI formatÄ±nda mesaj listesi
                      [{"role": "system"|"user"|"assistant", "content": "..."}]
        Returns:
            Asistan yanÄ±tÄ± (str).
        Raises:
            LLMConnectionError: BaÄŸlantÄ± veya API hatasÄ±.
        """
        ...

    @abstractmethod
    def is_available(self) -> bool:
        """Backend'in kullanÄ±labilir olup olmadÄ±ÄŸÄ±nÄ± kontrol et."""
        ...

    def list_models(self) -> List[str]:
        """Mevcut modellerin listesini dÃ¶ndÃ¼r (destekleniyorsa)."""
        return []

    def __repr__(self) -> str:
        return f"<{type(self).__name__} name={self.name!r}>"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Ollama Backend (Yerel)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class OllamaBackend(LLMBackend):
    """Yerel Ollama sunucusu Ã¼zerinden LLM Ã§aÄŸrÄ±sÄ±.

    VarsayÄ±lan olarak http://localhost:11434 adresini kullanÄ±r.
    """

    name = "ollama"

    def __init__(self, model: str = "qwen2.5:latest", host: Optional[str] = None):
        self.model = model
        self.host = host or os.getenv("OLLAMA_HOST", "http://localhost:11434")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        from exceptions import LLMConnectionError
        try:
            import ollama
            client = ollama.Client(host=self.host)
            response = client.chat(model=self.model, messages=messages)
            return response["message"]["content"]
        except ImportError:
            raise LLMConnectionError(
                self.model,
                details="ollama paketi bulunamadÄ±",
                suggestion="pip install ollama",
            )
        except Exception as e:
            raise LLMConnectionError(
                self.model, details=str(e),
                suggestion="Ollama servisinin Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun: ollama serve",
            )

    def is_available(self) -> bool:
        try:
            import ollama
            client = ollama.Client(host=self.host)
            client.list()
            return True
        except Exception:
            return False

    def list_models(self) -> List[str]:
        try:
            import ollama
            client = ollama.Client(host=self.host)
            models = client.list()
            return [m["name"] for m in models.get("models", [])]
        except Exception:
            return []


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  OpenAI Backend
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class OpenAIBackend(LLMBackend):
    """OpenAI API (GPT-4, GPT-3.5-turbo, vb.).

    API key: OPENAI_API_KEY ortam deÄŸiÅŸkeninden okunur.
    """

    name = "openai"

    def __init__(self, model: str = "gpt-4", api_key: Optional[str] = None):
        self.model = model
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        from exceptions import LLMConnectionError
        if not self.api_key:
            raise LLMConnectionError(
                self.model,
                details="OPENAI_API_KEY ortam deÄŸiÅŸkeni tanÄ±mlÄ± deÄŸil",
                suggestion="export OPENAI_API_KEY='sk-...' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.",
            )
        try:
            import openai
            client = openai.OpenAI(api_key=self.api_key)
            response = client.chat.completions.create(
                model=self.model,
                messages=messages,
                **kwargs,
            )
            return response.choices[0].message.content
        except ImportError:
            raise LLMConnectionError(
                self.model,
                details="openai paketi bulunamadÄ±",
                suggestion="pip install openai",
            )
        except Exception as e:
            raise LLMConnectionError(self.model, str(e))

    def is_available(self) -> bool:
        return bool(self.api_key)

    def list_models(self) -> List[str]:
        return ["gpt-4", "gpt-4-turbo", "gpt-3.5-turbo", "gpt-4o", "gpt-4o-mini"]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Anthropic Backend
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AnthropicBackend(LLMBackend):
    """Anthropic API (Claude 3, Claude 3.5, vb.).

    API key: ANTHROPIC_API_KEY ortam deÄŸiÅŸkeninden okunur.
    """

    name = "anthropic"

    def __init__(self, model: str = "claude-3-5-sonnet-20241022", api_key: Optional[str] = None):
        self.model = model
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        from exceptions import LLMConnectionError
        if not self.api_key:
            raise LLMConnectionError(
                self.model,
                details="ANTHROPIC_API_KEY ortam deÄŸiÅŸkeni tanÄ±mlÄ± deÄŸil",
                suggestion="export ANTHROPIC_API_KEY='sk-ant-...' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.",
            )
        try:
            import anthropic
            client = anthropic.Anthropic(api_key=self.api_key)

            # Anthropic API system mesajÄ± ayrÄ± parametre olarak alÄ±r
            system_msg = ""
            chat_messages = []
            for m in messages:
                if m["role"] == "system":
                    system_msg = m["content"]
                else:
                    chat_messages.append(m)

            response = client.messages.create(
                model=self.model,
                max_tokens=4096,
                system=system_msg,
                messages=chat_messages,
                **kwargs,
            )
            return response.content[0].text
        except ImportError:
            raise LLMConnectionError(
                self.model,
                details="anthropic paketi bulunamadÄ±",
                suggestion="pip install anthropic",
            )
        except Exception as e:
            raise LLMConnectionError(self.model, str(e))

    def is_available(self) -> bool:
        return bool(self.api_key)

    def list_models(self) -> List[str]:
        return [
            "claude-3-5-sonnet-20241022",
            "claude-3-5-haiku-20241022",
            "claude-3-opus-20240229",
        ]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Google Gemini Backend
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class GeminiBackend(LLMBackend):
    """Google Gemini API.

    API key: GEMINI_API_KEY ortam deÄŸiÅŸkeninden okunur.
    """

    name = "gemini"

    def __init__(self, model: str = "gemini-2.5-flash", api_key: Optional[str] = None):
        self.model = model
        self.api_key = api_key or os.getenv("GEMINI_API_KEY")

    def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        from exceptions import LLMConnectionError
        if not self.api_key:
            raise LLMConnectionError(
                model=self.model,
                message="GEMINI_API_KEY ortam deÄŸiÅŸkeni tanÄ±mlÄ± deÄŸil",
                details="GEMINI_API_KEY ortam deÄŸiÅŸkeni tanÄ±mlÄ± deÄŸil",
                suggestion="export GEMINI_API_KEY='...' komutunu Ã§alÄ±ÅŸtÄ±rÄ±n.",
            )
        try:
            from google import genai
            from google.genai import types
            
            client = genai.Client(api_key=self.api_key)

            # OpenAI formatÄ±nÄ± Gemini formatÄ±na Ã§evir
            history = []
            system_instruction = ""
            for m in messages:
                if m["role"] == "system":
                    system_instruction = m["content"]
                elif m["role"] == "user":
                    history.append(types.Content(role="user", parts=[types.Part.from_text(text=m["content"])]))
                elif m["role"] == "assistant":
                    history.append(types.Content(role="model", parts=[types.Part.from_text(text=m["content"])]))

            config = types.GenerateContentConfig(
                system_instruction=system_instruction if system_instruction else None,
            )

            if history:
                 last_msg = history.pop() # Son mesajÄ± alÄ±yoruz
                 if last_msg.role == "model":
                     # EÄŸer son mesaj asistan mesajÄ±ysa, history'ye geri ekleyip boÅŸ mesaj yollayamayÄ±z.
                     # KullanÄ±cÄ±dan gelen son mesaj olmasÄ± beklenir. EÄŸer deÄŸilse boÅŸ yollanÄ±r. (gemini buna izin vermeyebilir)
                     # BasitÃ§e tÃ¼m history'i verip 'Continue' diyebiliriz ama normal akÄ±ÅŸta son mesaj user olur.
                     history.append(last_msg)
                     last_msg_text = ""
                 else:
                     last_msg_text = last_msg.parts[0].text
            else:
                 last_msg_text = ""
                 
            chat = client.chats.create(model=self.model, config=config, history=history)
            response = chat.send_message(last_msg_text)
            return response.text
        except ImportError:
            raise LLMConnectionError(
                model=self.model,
                message="google-genai paketi bulunamadÄ±",
                details="google-genai paketi bulunamadÄ±",
                suggestion="pip install google-genai",
            )
        except Exception as e:
            raise LLMConnectionError(self.model, str(e))

    def is_available(self) -> bool:
        return bool(self.api_key)

    def list_models(self) -> List[str]:
        return ["gemini-2.5-flash", "gemini-2.5-pro", "gemini-2.0-flash-lite"]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Backend Registry & Factory
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

_BACKENDS: Dict[str, type] = {
    "ollama": OllamaBackend,
    "openai": OpenAIBackend,
    "anthropic": AnthropicBackend,
    "gemini": GeminiBackend,
}


def create_backend(name: str, **kwargs) -> LLMBackend:
    """Ä°sme gÃ¶re backend oluÅŸtur.

    Args:
        name: Backend adÄ± ("ollama", "openai", "anthropic", "gemini")
        **kwargs: Backend'e Ã¶zel parametreler (model, api_key, vb.)

    Returns:
        LLMBackend instance.

    Raises:
        ValueError: Bilinmeyen backend adÄ±.
    """
    cls = _BACKENDS.get(name.lower())
    if cls is None:
        available = ", ".join(sorted(_BACKENDS.keys()))
        raise ValueError(
            f"Bilinmeyen LLM backend: {name!r}. "
            f"Desteklenen backend'ler: {available}"
        )
    return cls(**kwargs)


def list_backends() -> List[str]:
    """Desteklenen backend isimlerini dÃ¶ndÃ¼r."""
    return sorted(_BACKENDS.keys())


def register_backend(name: str, cls: type) -> None:
    """Yeni bir backend kaydet.

    Args:
        name: Backend adÄ±.
        cls: LLMBackend alt sÄ±nÄ±fÄ±.
    """
    if not issubclass(cls, LLMBackend):
        raise TypeError(f"{cls!r} LLMBackend alt sÄ±nÄ±fÄ± olmalÄ±dÄ±r.")
    _BACKENDS[name.lower()] = cls


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Otomatik Backend SeÃ§imi (local / remote)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# Model adÄ± desenleri â†’ backend eÅŸleÅŸtirmesi
_MODEL_PATTERNS: Dict[str, str] = {
    "gpt-": "openai",
    "o1-": "openai",
    "o3-": "openai",
    "o4-": "openai",
    "chatgpt-": "openai",
    "claude-": "anthropic",
    "gemini-": "gemini",
}


def detect_backend_name(model: str) -> str:
    """Model adÄ±ndan backend ismini tahmin et.

    Args:
        model: Model adÄ± (Ã¶r: 'gpt-4o-mini', 'claude-3-5-sonnet-20241022', 'gemini-2.0-flash')

    Returns:
        Backend adÄ± ('openai', 'anthropic', 'gemini', 'ollama')
    """
    model_lower = model.lower().strip()
    for prefix, backend_name in _MODEL_PATTERNS.items():
        if model_lower.startswith(prefix):
            return backend_name
    return "ollama"


def auto_create_backend(model: str, mode: str = "auto") -> LLMBackend:
    """Model adÄ± ve moda gÃ¶re otomatik backend oluÅŸtur.

    Args:
        model: Model adÄ± (Ã¶r: 'gpt-4o-mini', 'qwen2.5:7b-instruct')
        mode:
            'local'  â†’ Her zaman Ollama kullan
            'remote' â†’ Model adÄ±ndan backend algÄ±la (gptâ†’OpenAI, claudeâ†’Anthropic, geminiâ†’Gemini)
            'auto'   â†’ Model adÄ± bulut saÄŸlayÄ±cÄ±sÄ±na benziyorsa remote, deÄŸilse local

    Returns:
        LLMBackend instance.
    """
    mode = mode.lower().strip()

    if mode == "local":
        log.info("ğŸ  Backend modu: LOCAL â†’ Ollama | model=%s", model)
        return OllamaBackend(model=model)

    # remote veya auto â†’ model adÄ±ndan backend belirle
    backend_name = detect_backend_name(model)

    if mode == "auto" and backend_name == "ollama":
        log.info("ğŸ  Backend modu: AUTO â†’ Ollama (yerel model) | model=%s", model)
        return OllamaBackend(model=model)

    if backend_name == "ollama" and mode == "remote":
        # KullanÄ±cÄ± remote dedi ama model adÄ± yerel gibi gÃ¶rÃ¼nÃ¼yor
        log.warning(
            "âš ï¸ Backend modu REMOTE ama model '%s' bir bulut modeline benzemiyor. "
            "Yine de Ollama ile denenecek. Bulut API kullanmak iÃ§in "
            "gpt-4o-mini / claude-3-5-sonnet-20241022 / gemini-2.0-flash gibi model adlarÄ± kullanÄ±n.",
            model,
        )
        return OllamaBackend(model=model)

    log.info("â˜ï¸  Backend modu: %s â†’ %s | model=%s", mode.upper(), backend_name.upper(), model)
    return create_backend(backend_name, model=model)

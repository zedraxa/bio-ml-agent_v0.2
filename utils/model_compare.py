# utils/model_compare.py
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Bio-ML Agent â€” Ã‡oklu Model KarÅŸÄ±laÅŸtÄ±rma ModÃ¼lÃ¼
#
#  KullanÄ±m:
#    from utils.model_compare import ModelComparator
#    comparator = ModelComparator(task_type="classification")
#    results = comparator.run(X_train, X_test, y_train, y_test)
#    comparator.save_results("results/")
#
#  Veya standalone:
#    python utils/model_compare.py --data data.csv --target quality
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import json
import time
import warnings
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
from sklearn.base import BaseEstimator, clone
from sklearn.ensemble import (
    GradientBoostingClassifier,
    GradientBoostingRegressor,
    RandomForestClassifier,
    RandomForestRegressor,
)
from sklearn.linear_model import (
    LinearRegression,
    LogisticRegression,
    Ridge,
)
from sklearn.metrics import (
    accuracy_score,
    classification_report,
    confusion_matrix,
    f1_score,
    mean_absolute_error,
    mean_squared_error,
    precision_score,
    r2_score,
    recall_score,
    roc_auc_score,
)
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC, SVR
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor

warnings.filterwarnings("ignore")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  VarsayÄ±lan Model Setleri
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_default_classifiers() -> Dict[str, BaseEstimator]:
    """VarsayÄ±lan sÄ±nÄ±flandÄ±rma modellerini dÃ¶ndÃ¼rÃ¼r."""
    return {
        "LogisticRegression": Pipeline([
            ("scaler", StandardScaler()),
            ("clf", LogisticRegression(max_iter=2000, solver="lbfgs")),
        ]),
        "RandomForest": Pipeline([
            ("scaler", StandardScaler()),
            ("clf", RandomForestClassifier(n_estimators=100, random_state=42)),
        ]),
        "GradientBoosting": Pipeline([
            ("scaler", StandardScaler()),
            ("clf", GradientBoostingClassifier(n_estimators=100, random_state=42)),
        ]),
        "SVM": Pipeline([
            ("scaler", StandardScaler()),
            ("clf", SVC(probability=True, random_state=42)),
        ]),
        "KNN": Pipeline([
            ("scaler", StandardScaler()),
            ("clf", KNeighborsClassifier(n_neighbors=5)),
        ]),
        "DecisionTree": Pipeline([
            ("scaler", StandardScaler()),
            ("clf", DecisionTreeClassifier(random_state=42)),
        ]),
    }


def get_default_regressors() -> Dict[str, BaseEstimator]:
    """VarsayÄ±lan regresyon modellerini dÃ¶ndÃ¼rÃ¼r."""
    return {
        "LinearRegression": Pipeline([
            ("scaler", StandardScaler()),
            ("reg", LinearRegression()),
        ]),
        "Ridge": Pipeline([
            ("scaler", StandardScaler()),
            ("reg", Ridge(alpha=1.0)),
        ]),
        "RandomForest": Pipeline([
            ("scaler", StandardScaler()),
            ("reg", RandomForestRegressor(n_estimators=100, random_state=42)),
        ]),
        "GradientBoosting": Pipeline([
            ("scaler", StandardScaler()),
            ("reg", GradientBoostingRegressor(n_estimators=100, random_state=42)),
        ]),
        "SVR": Pipeline([
            ("scaler", StandardScaler()),
            ("reg", SVR()),
        ]),
        "KNN": Pipeline([
            ("scaler", StandardScaler()),
            ("reg", KNeighborsRegressor(n_neighbors=5)),
        ]),
        "DecisionTree": Pipeline([
            ("scaler", StandardScaler()),
            ("reg", DecisionTreeRegressor(random_state=42)),
        ]),
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Model Sonucu Veri SÄ±nÄ±fÄ±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ModelResult:
    """Tek bir modelin eÄŸitim/test sonuÃ§larÄ±nÄ± tutar."""
    name: str
    metrics: Dict[str, float]
    train_time: float
    cv_scores: Optional[List[float]] = None
    cv_mean: Optional[float] = None
    cv_std: Optional[float] = None
    confusion_mat: Optional[List[List[int]]] = None
    classification_rep: Optional[str] = None

    def to_dict(self) -> Dict[str, Any]:
        """JSON-serializable dict'e dÃ¶nÃ¼ÅŸtÃ¼r."""
        d: Dict[str, Any] = {
            "model": self.name,
            "metrics": {k: round(v, 6) for k, v in self.metrics.items()},
            "train_time_seconds": round(self.train_time, 4),
        }
        if self.cv_mean is not None:
            d["cross_validation"] = {
                "scores": [round(s, 4) for s in (self.cv_scores or [])],
                "mean": round(self.cv_mean, 4),
                "std": round(self.cv_std or 0.0, 4),
            }
        if self.confusion_mat is not None:
            d["confusion_matrix"] = self.confusion_mat
        return d


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Ana KarÅŸÄ±laÅŸtÄ±rma SÄ±nÄ±fÄ±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ModelComparator:
    """
    Birden fazla ML modelini eÄŸitip karÅŸÄ±laÅŸtÄ±ran sÄ±nÄ±f.

    KullanÄ±m:
        comparator = ModelComparator(task_type="classification")
        results = comparator.run(X_train, X_test, y_train, y_test)
        comparator.print_comparison()
        comparator.save_results("results/")
    """

    def __init__(
        self,
        task_type: str = "classification",
        models: Optional[Dict[str, BaseEstimator]] = None,
        cv_folds: int = 5,
        random_state: int = 42,
    ):
        """
        Args:
            task_type: "classification" veya "regression"
            models: Ã–zel model sÃ¶zlÃ¼ÄŸÃ¼ (None ise varsayÄ±lanlar kullanÄ±lÄ±r)
            cv_folds: Ã‡apraz doÄŸrulama kat sayÄ±sÄ±
            random_state: Tekrarlanabilirlik iÃ§in seed
        """
        self.task_type = task_type.lower()
        if self.task_type not in ("classification", "regression"):
            raise ValueError(f"task_type 'classification' veya 'regression' olmalÄ±, '{task_type}' verildi")

        if models is not None:
            self.models = models
        elif self.task_type == "classification":
            self.models = get_default_classifiers()
        else:
            self.models = get_default_regressors()

        self.cv_folds = cv_folds
        self.random_state = random_state
        self.results: List[ModelResult] = []
        self.best_model_name: Optional[str] = None
        self.best_model: Optional[BaseEstimator] = None
        self._primary_metric: str = ""

    def run(
        self,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
        run_cv: bool = True,
    ) -> List[ModelResult]:
        """
        TÃ¼m modelleri eÄŸitip deÄŸerlendirir.

        Args:
            X_train, X_test, y_train, y_test: EÄŸitim ve test verileri
            run_cv: Ã‡apraz doÄŸrulama yapÄ±lsÄ±n mÄ±

        Returns:
            ModelResult listesi (en iyiden en kÃ¶tÃ¼ye sÄ±ralÄ±)
        """
        self.results = []

        for name, model in self.models.items():
            print(f"  â³ {name} eÄŸitiliyor...", end="", flush=True)
            result = self._train_and_evaluate(
                name, clone(model), X_train, X_test, y_train, y_test, run_cv
            )
            self.results.append(result)

            primary = self._get_primary_metric_value(result)
            print(f" âœ… ({self._primary_metric}: {primary:.4f}, sÃ¼re: {result.train_time:.2f}s)")

        # SonuÃ§larÄ± birincil metriÄŸe gÃ¶re sÄ±rala (en iyi ilk)
        self.results.sort(
            key=lambda r: r.metrics.get(self._primary_metric, 0),
            reverse=True,
        )

        # En iyi modeli belirle
        if self.results:
            self.best_model_name = self.results[0].name
            self.best_model = clone(self.models[self.best_model_name])
            self.best_model.fit(X_train, y_train)

        return self.results

    def _train_and_evaluate(
        self,
        name: str,
        model: BaseEstimator,
        X_train: np.ndarray,
        X_test: np.ndarray,
        y_train: np.ndarray,
        y_test: np.ndarray,
        run_cv: bool,
    ) -> ModelResult:
        """Tek bir modeli eÄŸit ve deÄŸerlendir."""

        # EÄŸitim
        start = time.time()
        model.fit(X_train, y_train)
        train_time = time.time() - start

        y_pred = model.predict(X_test)

        if self.task_type == "classification":
            metrics = self._classification_metrics(model, X_test, y_test, y_pred)
            self._primary_metric = "accuracy"

            # Confusion matrix
            cm = confusion_matrix(y_test, y_pred).tolist()
            cls_report = classification_report(y_test, y_pred, zero_division=0)
        else:
            metrics = self._regression_metrics(y_test, y_pred)
            self._primary_metric = "r2"
            cm = None
            cls_report = None

        # Ã‡apraz doÄŸrulama
        cv_scores = None
        cv_mean = None
        cv_std = None
        if run_cv:
            try:
                scoring = "accuracy" if self.task_type == "classification" else "r2"
                cv_results = cross_val_score(
                    clone(self.models[name]),
                    np.vstack([X_train, X_test]),
                    np.concatenate([y_train, y_test]),
                    cv=self.cv_folds,
                    scoring=scoring,
                )
                cv_scores = cv_results.tolist()
                cv_mean = float(cv_results.mean())
                cv_std = float(cv_results.std())
            except Exception:
                pass  # CV baÅŸarÄ±sÄ±z olursa sessizce atla

        return ModelResult(
            name=name,
            metrics=metrics,
            train_time=train_time,
            cv_scores=cv_scores,
            cv_mean=cv_mean,
            cv_std=cv_std,
            confusion_mat=cm,
            classification_rep=cls_report,
        )

    def _classification_metrics(
        self,
        model: BaseEstimator,
        X_test: np.ndarray,
        y_test: np.ndarray,
        y_pred: np.ndarray,
    ) -> Dict[str, float]:
        """SÄ±nÄ±flandÄ±rma metrikleri hesaplar."""
        n_classes = len(np.unique(y_test))
        average = "binary" if n_classes == 2 else "macro"

        metrics: Dict[str, float] = {
            "accuracy": float(accuracy_score(y_test, y_pred)),
            "precision": float(precision_score(y_test, y_pred, average=average, zero_division=0)),
            "recall": float(recall_score(y_test, y_pred, average=average, zero_division=0)),
            "f1": float(f1_score(y_test, y_pred, average=average, zero_division=0)),
        }

        # ROC-AUC (probability gerektirir)
        try:
            if hasattr(model, "predict_proba"):
                proba = model.predict_proba(X_test)
                if n_classes == 2:
                    metrics["roc_auc"] = float(roc_auc_score(y_test, proba[:, 1]))
                else:
                    metrics["roc_auc"] = float(
                        roc_auc_score(y_test, proba, multi_class="ovr", average="macro")
                    )
        except Exception:
            pass  # ROC-AUC hesaplanamadÄ±ysa atla

        return metrics

    def _regression_metrics(
        self,
        y_test: np.ndarray,
        y_pred: np.ndarray,
    ) -> Dict[str, float]:
        """Regresyon metrikleri hesaplar."""
        return {
            "r2": float(r2_score(y_test, y_pred)),
            "mae": float(mean_absolute_error(y_test, y_pred)),
            "mse": float(mean_squared_error(y_test, y_pred)),
            "rmse": float(np.sqrt(mean_squared_error(y_test, y_pred))),
        }

    def _get_primary_metric_value(self, result: ModelResult) -> float:
        """Birincil metrik deÄŸerini dÃ¶ndÃ¼rÃ¼r."""
        return result.metrics.get(self._primary_metric, 0.0)

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    #  Ã‡Ä±ktÄ± & Raporlama
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def print_comparison(self) -> str:
        """
        KarÅŸÄ±laÅŸtÄ±rma tablosunu konsola yazdÄ±rÄ±r ve string olarak dÃ¶ndÃ¼rÃ¼r.

        Returns:
            Tablo string'i
        """
        if not self.results:
            msg = "âŒ HenÃ¼z sonuÃ§ yok. Ã–nce run() Ã§aÄŸÄ±rÄ±n."
            print(msg)
            return msg

        lines: List[str] = []
        lines.append("")
        lines.append("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
        lines.append("â•‘                    ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rma SonuÃ§larÄ±                         â•‘")
        lines.append("â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£")

        if self.task_type == "classification":
            header = f"  {'#':<3} {'Model':<22} {'Accuracy':>9} {'Precision':>10} {'Recall':>8} {'F1':>8} {'ROC-AUC':>9} {'CV Mean':>8} {'SÃ¼re':>7}"
            lines.append(header)
            lines.append("  " + "â”€" * 90)

            for i, r in enumerate(self.results, 1):
                badge = " ğŸ†" if i == 1 else "   "
                auc_str = f"{r.metrics.get('roc_auc', 0):.4f}" if "roc_auc" in r.metrics else "  N/A "
                cv_str = f"{r.cv_mean:.4f}" if r.cv_mean is not None else "  N/A "
                line = (
                    f"  {i:<3}{badge}{r.name:<18} "
                    f"{r.metrics['accuracy']:>8.4f} "
                    f"{r.metrics['precision']:>9.4f} "
                    f"{r.metrics['recall']:>8.4f} "
                    f"{r.metrics['f1']:>7.4f} "
                    f"{auc_str:>8} "
                    f"{cv_str:>8} "
                    f"{r.train_time:>6.2f}s"
                )
                lines.append(line)
        else:
            header = f"  {'#':<3} {'Model':<22} {'RÂ²':>8} {'MAE':>10} {'RMSE':>10} {'CV Mean':>8} {'SÃ¼re':>7}"
            lines.append(header)
            lines.append("  " + "â”€" * 80)

            for i, r in enumerate(self.results, 1):
                badge = " ğŸ†" if i == 1 else "   "
                cv_str = f"{r.cv_mean:.4f}" if r.cv_mean is not None else "  N/A "
                line = (
                    f"  {i:<3}{badge}{r.name:<18} "
                    f"{r.metrics['r2']:>8.4f} "
                    f"{r.metrics['mae']:>9.4f} "
                    f"{r.metrics['rmse']:>9.4f} "
                    f"{cv_str:>8} "
                    f"{r.train_time:>6.2f}s"
                )
                lines.append(line)

        lines.append("")
        lines.append(f"  ğŸ† En Ä°yi Model: {self.best_model_name}")
        if self.results:
            best = self.results[0]
            primary_val = best.metrics.get(self._primary_metric, 0)
            lines.append(f"     {self._primary_metric}: {primary_val:.4f}")
            if best.cv_mean is not None:
                lines.append(f"     CV {self._primary_metric} (mean Â± std): {best.cv_mean:.4f} Â± {best.cv_std:.4f}")
        lines.append("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
        lines.append("")

        table_str = "\n".join(lines)
        print(table_str)
        return table_str

    def get_results_dict(self) -> Dict[str, Any]:
        """TÃ¼m sonuÃ§larÄ± JSON-serializable dict olarak dÃ¶ndÃ¼rÃ¼r."""
        return {
            "task_type": self.task_type,
            "n_models": len(self.results),
            "cv_folds": self.cv_folds,
            "primary_metric": self._primary_metric,
            "best_model": self.best_model_name,
            "ranking": [r.to_dict() for r in self.results],
        }

    def save_results(self, output_dir: str, prefix: str = "") -> Dict[str, Path]:
        """
        SonuÃ§larÄ± dosyalara kaydeder.

        Args:
            output_dir: Ã‡Ä±ktÄ± klasÃ¶rÃ¼
            prefix: Dosya adÄ± Ã¶neki (opsiyonel)

        Returns:
            OluÅŸturulan dosya yollarÄ±nÄ±n dict'i
        """
        out = Path(output_dir)
        out.mkdir(parents=True, exist_ok=True)

        pfx = f"{prefix}_" if prefix else ""
        saved: Dict[str, Path] = {}

        # 1. JSON â€” tÃ¼m sonuÃ§lar
        json_path = out / f"{pfx}comparison_results.json"
        json_path.write_text(
            json.dumps(self.get_results_dict(), ensure_ascii=False, indent=2),
            encoding="utf-8",
        )
        saved["json"] = json_path

        # 2. Markdown rapor
        md_path = out / f"{pfx}comparison_report.md"
        md_path.write_text(self._generate_markdown_report(), encoding="utf-8")
        saved["markdown"] = md_path

        # 3. CSV â€” metrik tablosu
        csv_path = out / f"{pfx}comparison_metrics.csv"
        self._save_metrics_csv(csv_path)
        saved["csv"] = csv_path

        print(f"ğŸ’¾ SonuÃ§lar kaydedildi:")
        for label, path in saved.items():
            print(f"   ğŸ“„ {label}: {path}")

        return saved

    def _save_metrics_csv(self, path: Path) -> None:
        """Metrik tablosunu CSV olarak kaydeder."""
        rows = []
        for r in self.results:
            row = {"model": r.name, "train_time_s": round(r.train_time, 4)}
            row.update({k: round(v, 6) for k, v in r.metrics.items()})
            if r.cv_mean is not None:
                row["cv_mean"] = round(r.cv_mean, 4)
                row["cv_std"] = round(r.cv_std or 0, 4)
            rows.append(row)
        pd.DataFrame(rows).to_csv(path, index=False)

    def _generate_markdown_report(self) -> str:
        """Markdown formatÄ±nda karÅŸÄ±laÅŸtÄ±rma raporu oluÅŸturur."""
        lines: List[str] = []
        lines.append("# ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rma Raporu\n")
        lines.append(f"**GÃ¶rev TÃ¼rÃ¼:** {self.task_type.capitalize()}")
        lines.append(f"**KarÅŸÄ±laÅŸtÄ±rÄ±lan Model SayÄ±sÄ±:** {len(self.results)}")
        lines.append(f"**Ã‡apraz DoÄŸrulama:** {self.cv_folds}-fold")
        lines.append(f"**En Ä°yi Model:** ğŸ† **{self.best_model_name}**\n")

        # SonuÃ§ tablosu
        lines.append("## KarÅŸÄ±laÅŸtÄ±rma Tablosu\n")

        if self.task_type == "classification":
            lines.append("| # | Model | Accuracy | Precision | Recall | F1 | ROC-AUC | CV Mean | SÃ¼re |")
            lines.append("|---|-------|----------|-----------|--------|-----|---------|---------|------|")
            for i, r in enumerate(self.results, 1):
                badge = "ğŸ† " if i == 1 else ""
                auc = f"{r.metrics.get('roc_auc', 0):.4f}" if "roc_auc" in r.metrics else "N/A"
                cv = f"{r.cv_mean:.4f}" if r.cv_mean is not None else "N/A"
                lines.append(
                    f"| {i} | {badge}**{r.name}** | "
                    f"{r.metrics['accuracy']:.4f} | "
                    f"{r.metrics['precision']:.4f} | "
                    f"{r.metrics['recall']:.4f} | "
                    f"{r.metrics['f1']:.4f} | "
                    f"{auc} | {cv} | "
                    f"{r.train_time:.2f}s |"
                )
        else:
            lines.append("| # | Model | RÂ² | MAE | RMSE | CV Mean | SÃ¼re |")
            lines.append("|---|-------|----|-----|------|---------|------|")
            for i, r in enumerate(self.results, 1):
                badge = "ğŸ† " if i == 1 else ""
                cv = f"{r.cv_mean:.4f}" if r.cv_mean is not None else "N/A"
                lines.append(
                    f"| {i} | {badge}**{r.name}** | "
                    f"{r.metrics['r2']:.4f} | "
                    f"{r.metrics['mae']:.4f} | "
                    f"{r.metrics['rmse']:.4f} | "
                    f"{cv} | "
                    f"{r.train_time:.2f}s |"
                )

        # En iyi model detaylarÄ±
        if self.results:
            best = self.results[0]
            lines.append(f"\n## ğŸ† En Ä°yi Model: {best.name}\n")
            lines.append("### Metrikler\n")
            for k, v in best.metrics.items():
                lines.append(f"- **{k}:** {v:.4f}")
            if best.cv_mean is not None:
                lines.append(f"- **CV Mean Â± Std:** {best.cv_mean:.4f} Â± {best.cv_std:.4f}")
            lines.append(f"- **EÄŸitim SÃ¼resi:** {best.train_time:.2f}s")

            if best.classification_rep:
                lines.append("\n### Classification Report\n")
                lines.append("```")
                lines.append(best.classification_rep)
                lines.append("```")

        lines.append("\n---")
        lines.append("*Bu rapor Bio-ML Agent tarafÄ±ndan otomatik oluÅŸturulmuÅŸtur.*\n")

        return "\n".join(lines)

    def plot_comparison(self, output_dir: str, prefix: str = "") -> Optional[Path]:
        """
        Model karÅŸÄ±laÅŸtÄ±rma grafiÄŸini oluÅŸturur ve kaydeder.

        Args:
            output_dir: Ã‡Ä±ktÄ± klasÃ¶rÃ¼
            prefix: Dosya adÄ± Ã¶neki

        Returns:
            Grafik dosyasÄ±nÄ±n yolu (matplotlib yoksa None)
        """
        try:
            import matplotlib
            matplotlib.use("Agg")
            import matplotlib.pyplot as plt
        except ImportError:
            print("âš ï¸ matplotlib yÃ¼klÃ¼ deÄŸil, grafik oluÅŸturulamadÄ±.")
            print("   YÃ¼klemek iÃ§in: pip install matplotlib")
            return None

        if not self.results:
            return None

        out = Path(output_dir)
        out.mkdir(parents=True, exist_ok=True)
        pfx = f"{prefix}_" if prefix else ""

        names = [r.name for r in self.results]
        n_models = len(names)

        if self.task_type == "classification":
            metrics_to_plot = ["accuracy", "precision", "recall", "f1"]
            available = [m for m in metrics_to_plot if m in self.results[0].metrics]

            fig, axes = plt.subplots(1, len(available), figsize=(5 * len(available), 6))
            if len(available) == 1:
                axes = [axes]

            colors = plt.cm.Set2(np.linspace(0, 1, n_models))

            for ax, metric in zip(axes, available):
                values = [r.metrics.get(metric, 0) for r in self.results]
                bars = ax.barh(names, values, color=colors, edgecolor="white", linewidth=0.5)

                # DeÄŸer etiketleri
                for bar, val in zip(bars, values):
                    ax.text(
                        bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,
                        f"{val:.4f}", va="center", fontsize=9, fontweight="bold",
                    )

                ax.set_xlabel(metric.capitalize(), fontsize=11)
                ax.set_xlim(0, max(values) * 1.15 if max(values) > 0 else 1)
                ax.set_title(metric.upper(), fontsize=13, fontweight="bold")
                ax.invert_yaxis()

            fig.suptitle("ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rmasÄ± â€” SÄ±nÄ±flandÄ±rma", fontsize=15, fontweight="bold", y=1.02)
        else:
            fig, axes = plt.subplots(1, 3, figsize=(16, 6))
            colors = plt.cm.Set2(np.linspace(0, 1, n_models))

            for ax, metric in zip(axes, ["r2", "mae", "rmse"]):
                values = [r.metrics.get(metric, 0) for r in self.results]
                bars = ax.barh(names, values, color=colors, edgecolor="white", linewidth=0.5)
                for bar, val in zip(bars, values):
                    ax.text(
                        bar.get_width() + 0.005, bar.get_y() + bar.get_height() / 2,
                        f"{val:.4f}", va="center", fontsize=9, fontweight="bold",
                    )
                ax.set_xlabel(metric.upper(), fontsize=11)
                ax.set_title(metric.upper(), fontsize=13, fontweight="bold")
                ax.invert_yaxis()

            fig.suptitle("ğŸ“Š Model KarÅŸÄ±laÅŸtÄ±rmasÄ± â€” Regresyon", fontsize=15, fontweight="bold", y=1.02)

        plt.tight_layout()
        chart_path = out / f"{pfx}comparison_chart.png"
        fig.savefig(chart_path, dpi=150, bbox_inches="tight", facecolor="white")
        plt.close(fig)

        print(f"ğŸ“Š Grafik kaydedildi: {chart_path}")
        return chart_path


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  KolaylÄ±k Fonksiyonu (Tek Ã‡aÄŸrÄ±da KarÅŸÄ±laÅŸtÄ±rma)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def compare_models(
    X_train: np.ndarray,
    X_test: np.ndarray,
    y_train: np.ndarray,
    y_test: np.ndarray,
    task_type: str = "classification",
    models: Optional[Dict[str, BaseEstimator]] = None,
    output_dir: Optional[str] = None,
    cv_folds: int = 5,
) -> Tuple[ModelComparator, List[ModelResult]]:
    """
    Tek Ã§aÄŸrÄ±da Ã§oklu model karÅŸÄ±laÅŸtÄ±rmasÄ± yapar.

    KullanÄ±m:
        comparator, results = compare_models(
            X_train, X_test, y_train, y_test,
            task_type="classification",
            output_dir="results/"
        )

    Returns:
        (ModelComparator, results listesi) tuple'Ä±
    """
    comparator = ModelComparator(
        task_type=task_type,
        models=models,
        cv_folds=cv_folds,
    )

    print(f"\nğŸ”¬ Ã‡oklu Model KarÅŸÄ±laÅŸtÄ±rmasÄ± ({task_type})")
    print(f"   Modeller: {', '.join(comparator.models.keys())}")
    print(f"   CV Folds: {cv_folds}")
    print("â”€" * 60)

    results = comparator.run(X_train, X_test, y_train, y_test)
    comparator.print_comparison()

    if output_dir:
        comparator.save_results(output_dir)
        comparator.plot_comparison(output_dir)

    return comparator, results


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  Standalone CLI
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    import argparse
    from sklearn.model_selection import train_test_split

    parser = argparse.ArgumentParser(description="Ã‡oklu Model KarÅŸÄ±laÅŸtÄ±rma AracÄ±")
    parser.add_argument("--data", required=True, help="CSV veri seti yolu")
    parser.add_argument("--target", required=True, help="Hedef sÃ¼tun adÄ±")
    parser.add_argument("--task", default="classification", choices=["classification", "regression"])
    parser.add_argument("--test-size", type=float, default=0.2)
    parser.add_argument("--cv-folds", type=int, default=5)
    parser.add_argument("--output", default="results", help="Ã‡Ä±ktÄ± klasÃ¶rÃ¼")
    parser.add_argument("--sep", default=",", help="CSV ayÄ±rÄ±cÄ± karakter")
    args = parser.parse_args()

    print(f"ğŸ“‚ Veri yÃ¼kleniyor: {args.data}")
    df = pd.read_csv(args.data, sep=args.sep)
    print(f"   Boyut: {df.shape[0]} satÄ±r Ã— {df.shape[1]} sÃ¼tun")

    X = df.drop(columns=[args.target]).values
    y = df[args.target].values

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=args.test_size, random_state=42
    )

    compare_models(
        X_train, X_test, y_train, y_test,
        task_type=args.task,
        output_dir=args.output,
        cv_folds=args.cv_folds,
    )
